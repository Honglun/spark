/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.spark.sql.execution.exchange

import org.apache.spark.sql.catalyst.expressions._
import org.apache.spark.sql.catalyst.plans.physical._
import org.apache.spark.sql.catalyst.rules.Rule
import org.apache.spark.sql.execution._
import org.apache.spark.sql.execution.joins.{ShuffledHashJoinExec, SortMergeJoinExec}
import org.apache.spark.sql.internal.SQLConf

import scala.collection.mutable
import scala.collection.mutable.ArrayBuffer

/**
  * Ensures that the [[org.apache.spark.sql.catalyst.plans.physical.Partitioning Partitioning]]
  * of input data meets the
  * [[org.apache.spark.sql.catalyst.plans.physical.Distribution Distribution]] requirements for
  * each operator by inserting [[ShuffleExchangeExec]] Operators where required.  Also ensure that
  * the input partition ordering requirements are met.
  */
case class EnsureRequirements(conf: SQLConf) extends Rule[SparkPlan] {
  private def defaultNumPreShufflePartitions: Int = conf.numShufflePartitions

  private def targetPostShuffleInputSize: Long = conf.targetPostShuffleInputSize

  private def adaptiveExecutionEnabled: Boolean = conf.adaptiveExecutionEnabled

  private def minNumPostShufflePartitions: Option[Int] = {
    val minNumPostShufflePartitions = conf.minNumPostShufflePartitions
    if (minNumPostShufflePartitions > 0) Some(minNumPostShufflePartitions) else None
  }

  /**
    * Adds [[ExchangeCoordinator]] to [[ShuffleExchangeExec]]s if adaptive query execution is enabled
    * and partitioning schemes of these [[ShuffleExchangeExec]]s support [[ExchangeCoordinator]].
    */
  private def withExchangeCoordinator(
                                       children: Seq[SparkPlan],
                                       requiredChildDistributions: Seq[Distribution]): Seq[SparkPlan] = {
    // 判断是否支持coordinator
    // 目前ExchangeCoordinator只支持HashPartitionings
    // 针对我们前面分析的SortMerge，children都是ShuffleExchangeExec(hash: HashPartitioning, _, _)，所以是支持的
    val supportsCoordinator =
    if (children.exists(_.isInstanceOf[ShuffleExchangeExec])) {
      // Right now, ExchangeCoordinator only support HashPartitionings.
      children.forall {
        case e @ ShuffleExchangeExec(hash: HashPartitioning, _, _) => true
        case child =>
          child.outputPartitioning match {
            case hash: HashPartitioning => true
            case collection: PartitioningCollection =>
              collection.partitionings.forall(_.isInstanceOf[HashPartitioning])
            case _ => false
          }
      }
    } else {
      // 还有一些情况，比如我们在两个子查询使用了ClusteredDistribution，但是没有Exchange操作
      // 因为这个时候可能会出现children的分区分布不一致，所以需要
      // In this case, although we do not have Exchange operators, we may still need to
      // shuffle data when we have more than one children because data generated by
      // these children may not be partitioned in the same way.
      // Please see the comment in withCoordinator for more details.
      val supportsDistribution = requiredChildDistributions.forall { dist =>
        dist.isInstanceOf[ClusteredDistribution] || dist.isInstanceOf[HashClusteredDistribution]
      }
      children.length > 1 && supportsDistribution
    }

    val withCoordinator =
      if (adaptiveExecutionEnabled && supportsCoordinator) {
        val coordinator =
          new ExchangeCoordinator(
            targetPostShuffleInputSize,
            minNumPostShufflePartitions)
        children.zip(requiredChildDistributions).map {
          case (e: ShuffleExchangeExec, _) =>
            // This child is an Exchange, we need to add the coordinator.
            e.copy(coordinator = Some(coordinator))
          case (child, distribution) =>
            // If this child is not an Exchange, we need to add an Exchange for now.
            // Ideally, we can try to avoid this Exchange. However, when we reach here,
            // there are at least two children operators (because if there is a single child
            // and we can avoid Exchange, supportsCoordinator will be false and we
            // will not reach here.). Although we can make two children have the same number of
            // post-shuffle partitions. Their numbers of pre-shuffle partitions may be different.
            // For example, let's say we have the following plan
            //         Join
            //         /  \
            //       Agg  Exchange
            //       /      \
            //    Exchange  t2
            //      /
            //     t1
            // In this case, because a post-shuffle partition can include multiple pre-shuffle
            // partitions, a HashPartitioning will not be strictly partitioned by the hashcodes
            // after shuffle. So, even we can use the child Exchange operator of the Join to
            // have a number of post-shuffle partitions that matches the number of partitions of
            // Agg, we cannot say these two children are partitioned in the same way.
            // Here is another case
            //         Join
            //         /  \
            //       Agg1  Agg2
            //       /      \
            //   Exchange1  Exchange2
            //       /       \
            //      t1       t2
            // In this case, two Aggs shuffle data with the same column of the join condition.
            // After we use ExchangeCoordinator, these two Aggs may not be partitioned in the same
            // way. Let's say that Agg1 and Agg2 both have 5 pre-shuffle partitions and 2
            // post-shuffle partitions. It is possible that Agg1 fetches those pre-shuffle
            // partitions by using a partitionStartIndices [0, 3]. However, Agg2 may fetch its
            // pre-shuffle partitions by using another partitionStartIndices [0, 4].
            // So, Agg1 and Agg2 are actually not co-partitioned.
            //
            // It will be great to introduce a new Partitioning to represent the post-shuffle
            // partitions when one post-shuffle partition includes multiple pre-shuffle partitions.
            val targetPartitioning = distribution.createPartitioning(defaultNumPreShufflePartitions)
            assert(targetPartitioning.isInstanceOf[HashPartitioning])
            ShuffleExchangeExec(targetPartitioning, child, Some(coordinator))
        }
      } else {
        // If we do not need ExchangeCoordinator, the original children are returned.
        children
      }

    withCoordinator
  }

  private def ensureDistributionAndOrdering(operator: SparkPlan): SparkPlan = {
    // scalastyle:off
    /**
      * 下面以SortMergeJoin为例
      * select t1.* from tmp_db.test_part02 t1 left join tmp_db.test_part01 t2 on(t1.id=t2.id)
      * sparkPlan 是这样的
      *
      * Project [id#8, username#9, pt#10]
      * +- SortMergeJoin [id#8], [id#11], LeftOuter
      *    :- Scan hive tmp_db.test_part02 [id#8, username#9, pt#10], HiveTableRelation `tmp_db`.`test_part02`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [id#8, username#9, pt#10]
      *    +- Filter isnotnull(id#11)
      *       +- Scan hive tmp_db.test_part01 [id#11], HiveTableRelation `tmp_db`.`test_part01`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [id#11, username#12], [pt#13]
      *
      *
      *    注意，这里的Scan是HiveTableScanExec，Filter是FilterExec
      */
    // 另外要清楚，能进入到这个方法里来，都是SparkPlan，我们上面的例子，实际上就是 SortMergeJoin 才能进来
    // 左右两边孩子的key的分布，Seq(HashClusteredDistribution(leftKeys),HashClusteredDistribution(rightKeys))
    val requiredChildDistributions: Seq[Distribution] = operator.requiredChildDistribution
    // 对join的key分别排序再合并成一个列表，Seq(requiredOrders(leftKeys),requiredOrders(rightKeys))
    val requiredChildOrderings: Seq[Seq[SortOrder]] = operator.requiredChildOrdering
    // 左右两边的孩子（HiveTableScanExec、FilterExec）
    var children: Seq[SparkPlan] = operator.children
    assert(requiredChildDistributions.length == children.length)
    assert(requiredChildOrderings.length == children.length)

    // Ensure that the operator's children satisfy their output distribution requirements.
    children = children.zip(requiredChildDistributions).map {
      // (HiveTableScanExec,HashClusteredDistribution(leftKeys)),(FilterExec,HashClusteredDistribution(rightKeys))
      // HiveTableScanExec没有实现outputPartitioning方法，它使用的是父类SparkPlan的outputPartitioning方法，实际上就是UnknownPartitioning，
      // UnknownPartitioning.satisfies(HashClusteredDistribution)肯定是false
      // FilterExec的outputPartitioning实际上是child的outputPartitioning，也就是HiveTableScanExec的，返回结果会与上面的一样
      case (child, distribution) if child.outputPartitioning.satisfies(distribution) =>
        child
      case (child, BroadcastDistribution(mode)) =>
        BroadcastExchangeExec(mode, child)
      case (child, distribution) =>
        // 最后，会走到这里来
        // 首先会计算分区的值 numPartitions
        // requiredNumPartitions默认是空，所以会取defaultNumPreShufflePartitions，它是获取的我们熟悉的配置spark.sql.shuffle.partitions的值
        val numPartitions = distribution.requiredNumPartitions
          .getOrElse(defaultNumPreShufflePartitions)
        //返回一个ShuffleExchangeExec对象，第一个参数是HashPartitioning(keys, numPartitions)
        ShuffleExchangeExec(distribution.createPartitioning(numPartitions), child)
    }
    // 到这里，执行计划就多了一层，变成了这样
    // *(4) Project [id#8, username#9, pt#10]
    //  +- SortMergeJoin [id#8], [id#11], LeftOuter
    //   :+- Exchange hashpartitioning(id#8, 3)
    //   :   +- Scan hive tmp_db.test_part02 [id#8, username#9, pt#10], HiveTableRelation `tmp_db`.`test_part02`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [id#8, username#9, pt#10]
    //    +- Exchange hashpartitioning(id#11, 3)
    //       +- *(2) Filter isnotnull(id#11)
    //         +- Scan hive tmp_db.test_part01 [id#11], HiveTableRelation `tmp_db`.`test_part01`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [id#11, username#12], [pt#13]

    //children 也就变成了两个ShuffleExchangeExec了

    // Get the indexes of children which have specified distribution requirements and need to have
    // same number of partitions.
    // 获取children的索引，这些children有指定的分布要求，并且需要有相同数量的分区
    // 索引从0开始，这里返回的是[0,1]
    val childrenIndexes = requiredChildDistributions.zipWithIndex.filter {
      case (UnspecifiedDistribution, _) => false
      case (_: BroadcastDistribution, _) => false
      case _ => true
    }.map(_._2)

    // ShuffleExchangeExec的outputPartitioning就是上面的第一个参数HashPartitioning(keys, numPartitions)
    // 明显，假设我们设置的参数spark.sql.shuffle.partitions=3，那么childrenNumPartitions就是Set(3,3)了
    val childrenNumPartitions =
    childrenIndexes.map(children(_).outputPartitioning.numPartitions).toSet

    // 这里判断的目的不知道是做什么，requiredChildDistributions的requiredNumPartitions都是None，所以符合下面的判断
    if (childrenNumPartitions.size > 1) {
      // Get the number of partitions which is explicitly required by the distributions.
      // 初步看出来是为了验证 requiredChildDistributions 两边的Distribution的requiredNumPartitions为空，或者一样
      // 因为下面的toSet会去重
      // 最后会返回set的第一个元素，也是期望的唯一的一个值，如果有的话
      val requiredNumPartitions = {
        val numPartitionsSet = childrenIndexes.flatMap {
          index => requiredChildDistributions(index).requiredNumPartitions
        }.toSet
        assert(numPartitionsSet.size <= 1,
          s"$operator have incompatible requirements of the number of partitions for its children")
        numPartitionsSet.headOption
      }

      // 这里那就是3了
      // 这里其实有一些推论，我们可以看到上面的partition数量默认会取defaultNumPreShufflePartitions，也就是我们配置的
      // 如果取的是默认的，那么这个targetNumPartitions肯定也会等于默认的
      // 如果有不一样的情况，那就是distribution.requiredNumPartitions引起的，目前来看SortMerge是没有这个值的
      // 后面看其它shuffle会不会有这个值的配置
      val targetNumPartitions = requiredNumPartitions.getOrElse(childrenNumPartitions.max)

      //这里的目的就是统一两边child的partition数量，取最大的那个
      children = children.zip(requiredChildDistributions).zipWithIndex.map {
        case ((child, distribution), index) if childrenIndexes.contains(index) =>
          if (child.outputPartitioning.numPartitions == targetNumPartitions) {
            child
          } else {
            val defaultPartitioning = distribution.createPartitioning(targetNumPartitions)
            child match {
              // If child is an exchange, we replace it with a new one having defaultPartitioning.
              case ShuffleExchangeExec(_, c, _) => ShuffleExchangeExec(defaultPartitioning, c)
              case _ => ShuffleExchangeExec(defaultPartitioning, child)
            }
          }

        case ((child, _), _) => child
      }
    }

    // Now, we need to add ExchangeCoordinator if necessary.
    // Actually, it is not a good idea to add ExchangeCoordinators while we are adding Exchanges.
    // However, with the way that we plan the query, we do not have a place where we have a
    // global picture of all shuffle dependencies of a post-shuffle stage. So, we add coordinator
    // at here for now.
    // Once we finish https://issues.apache.org/jira/browse/SPARK-10665,
    // we can first add Exchanges and then add coordinator once we have a DAG of query fragments.
    children = withExchangeCoordinator(children, requiredChildDistributions)

    // Now that we've performed any necessary shuffles, add sorts to guarantee output orderings:
    children = children.zip(requiredChildOrderings).map { case (child, requiredOrdering) =>
      // If child.outputOrdering already satisfies the requiredOrdering, we do not need to sort.
      // child是ShuffleExchangeExec，它的outputOrdering是父类SparkPlan的，也就是Nil
      // requiredOrdering是requiredOrders(leftKeys)和requiredOrders(rightKeys)
      // 明显是匹配不上的
      if (SortOrder.orderingSatisfies(child.outputOrdering, requiredOrdering)) {
        child
      } else {
        // 最后会走这一步，又包装了一层SortExec
        SortExec(requiredOrdering, global = false, child = child)
      }
    }
    //执行计划最后是这样的
//    *(4) Project [id#303978, username#303979, pt#303980]
//    +- SortMergeJoin [id#303978], [id#303981], LeftOuter
//       :- *(1) Sort [id#303978 ASC NULLS FIRST], false, 0
//       :  +- Exchange(coordinator id: 24943937) hashpartitioning(id#303978, 4096), coordinator[target post-shuffle partition size: 268435456]
//       :     +- Scan hive tmp_db.test_part02 [id#303978, username#303979, pt#303980], HiveTableRelation `tmp_db`.`test_part02`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [id#303978, username#303979, pt#303980]
//       +- *(3) Sort [id#303981 ASC NULLS FIRST], false, 0
//          +- Exchange(coordinator id: 24943937) hashpartitioning(id#303981, 4096), coordinator[target post-shuffle partition size: 268435456]
//             +- *(2) Filter isnotnull(id#303981)
//                +- Scan hive tmp_db.test_part01 [id#303981], HiveTableRelation `tmp_db`.`test_part01`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [id#303981, username#303982], [pt#303983]  |

    operator.withNewChildren(children)
  }

  private def reorder(
                       leftKeys: Seq[Expression],
                       rightKeys: Seq[Expression],
                       expectedOrderOfKeys: Seq[Expression],
                       currentOrderOfKeys: Seq[Expression]): (Seq[Expression], Seq[Expression]) = {
    val leftKeysBuffer = ArrayBuffer[Expression]()
    val rightKeysBuffer = ArrayBuffer[Expression]()
    val pickedIndexes = mutable.Set[Int]()
    val keysAndIndexes = currentOrderOfKeys.zipWithIndex

    expectedOrderOfKeys.foreach(expression => {
      val index = keysAndIndexes.find { case (e, idx) =>
        // As we may have the same key used many times, we need to filter out its occurrence we
        // have already used.
        e.semanticEquals(expression) && !pickedIndexes.contains(idx)
      }.map(_._2).get
      pickedIndexes += index
      leftKeysBuffer.append(leftKeys(index))
      rightKeysBuffer.append(rightKeys(index))
    })
    (leftKeysBuffer, rightKeysBuffer)
  }

  private def reorderJoinKeys(
                               leftKeys: Seq[Expression],
                               rightKeys: Seq[Expression],
                               leftPartitioning: Partitioning,
                               rightPartitioning: Partitioning): (Seq[Expression], Seq[Expression]) = {
    if (leftKeys.forall(_.deterministic) && rightKeys.forall(_.deterministic)) {
      leftPartitioning match {
        case HashPartitioning(leftExpressions, _)
          if leftExpressions.length == leftKeys.length &&
            leftKeys.forall(x => leftExpressions.exists(_.semanticEquals(x))) =>
          reorder(leftKeys, rightKeys, leftExpressions, leftKeys)

        case _ => rightPartitioning match {
          case HashPartitioning(rightExpressions, _)
            if rightExpressions.length == rightKeys.length &&
              rightKeys.forall(x => rightExpressions.exists(_.semanticEquals(x))) =>
            reorder(leftKeys, rightKeys, rightExpressions, rightKeys)

          case _ => (leftKeys, rightKeys)
        }
      }
    } else {
      (leftKeys, rightKeys)
    }
  }

  /**
    * When the physical operators are created for JOIN, the ordering of join keys is based on order
    * in which the join keys appear in the user query. That might not match with the output
    * partitioning of the join node's children (thus leading to extra sort / shuffle being
    * introduced). This rule will change the ordering of the join keys to match with the
    * partitioning of the join nodes' children.
    */
  private def reorderJoinPredicates(plan: SparkPlan): SparkPlan = {
    plan match {
      case ShuffledHashJoinExec(leftKeys, rightKeys, joinType, buildSide, condition, left, right) =>
        val (reorderedLeftKeys, reorderedRightKeys) =
          reorderJoinKeys(leftKeys, rightKeys, left.outputPartitioning, right.outputPartitioning)
        ShuffledHashJoinExec(reorderedLeftKeys, reorderedRightKeys, joinType, buildSide, condition,
          left, right)

      case SortMergeJoinExec(leftKeys, rightKeys, joinType, condition, left, right) =>
        val (reorderedLeftKeys, reorderedRightKeys) =
          reorderJoinKeys(leftKeys, rightKeys, left.outputPartitioning, right.outputPartitioning)
        SortMergeJoinExec(reorderedLeftKeys, reorderedRightKeys, joinType, condition, left, right)

      case other => other
    }
  }

  def apply(plan: SparkPlan): SparkPlan = plan.transformUp {
    // TODO: remove this after we create a physical operator for `RepartitionByExpression`.
    case operator @ ShuffleExchangeExec(upper: HashPartitioning, child, _) =>
      child.outputPartitioning match {
        case lower: HashPartitioning if upper.semanticEquals(lower) => child
        case _ => operator
      }
    case operator: SparkPlan =>
      ensureDistributionAndOrdering(reorderJoinPredicates(operator))
  }
}
